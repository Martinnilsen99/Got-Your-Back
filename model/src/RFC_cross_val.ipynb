{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Import",
   "metadata": {
    "tags": [],
    "cell_id": "00000-88445027-1734-4d6c-8cf9-657ebc34d7df",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ee38c6e2",
    "execution_millis": 1846,
    "cell_id": "00001-ad3dcf47-12d8-4e26-a1af-098531c0289c",
    "execution_start": 1621031534932,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "import sys; sys.path.insert(0, '..')\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom tabulate import tabulate\nfrom utils.df_utils import df_wrapper\nfrom dataclasses import dataclass\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import shuffle\nfrom utils.declarations import training_files, testing_files, POSE_MAP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Define data",
   "metadata": {
    "tags": [],
    "cell_id": "00002-625f790e-602d-455c-8179-2f5e9c2789b6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c71602b4",
    "execution_millis": 8,
    "cell_id": "00003-0833b981-1545-43c1-b6c8-4eae3fd75ec9",
    "execution_start": 1621031536781,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "AMOUNT_OF_SENSORS = 2\nVALIDATION_TESTSET = \"004\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Datapreparation",
   "metadata": {
    "tags": [],
    "cell_id": "00004-8901d0bd-7f04-4e66-9d89-b09e4a9ae9e0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Train",
   "metadata": {
    "tags": [],
    "cell_id": "00005-55a3e10a-e048-4c76-a001-5c6b626a5374",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dd88c00f",
    "execution_millis": 125448,
    "is_code_hidden": false,
    "cell_id": "00006-f655b5b2-7217-456f-9d08-2dc8bb3c4099",
    "execution_start": 1621031536790,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "x_train_arr = []\nfor key in training_files:\n    elem = df_wrapper(training_files[key].csv_file)\n    elem.concat_sensor_data(AMOUNT_OF_SENSORS)\n    elem.align_poses(training_files[key].annot_file, POSE_MAP)\n    elem.df.sample(frac=1)\n    x_train_arr.append(elem)\n\nx_train_arr = shuffle(x_train_arr, random_state=42)\nx_train = pd.concat([x.df.drop([' TimeStamp (s)', 'Pose'],axis=1) for x in x_train_arr])\ny_train = pd.concat([x.df['Pose'] for x in x_train_arr])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Test\n",
   "metadata": {
    "tags": [],
    "cell_id": "00007-5bb6c308-cea7-4bc7-8f0c-884657f7db7f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1e48518c",
    "execution_millis": 8175,
    "cell_id": "00008-7650808e-7c15-4f3f-b825-026d42028982",
    "execution_start": 1621031662238,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "x_test_dict = dict()\ny_test_dict = dict()\nfor key in testing_files:\n    elem = df_wrapper(testing_files[key].csv_file)\n    elem.concat_sensor_data(AMOUNT_OF_SENSORS)\n    elem.align_poses(testing_files[key].annot_file, POSE_MAP)\n    y_test = elem.df[\"Pose\"]\n    y_test.index = [i for i in range(len(y_test))]\n    x_test_dict[key] = elem.df\n    y_test_dict[key] = y_test\n\nx_test = x_test_dict[VALIDATION_TESTSET].drop([' TimeStamp (s)', 'Pose'], axis=1) \ny_test = y_test_dict[VALIDATION_TESTSET]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualisation",
   "metadata": {
    "tags": [],
    "cell_id": "00009-42d75bea-358c-41ec-bd41-03256a42d5ca",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6e5d68f6",
    "execution_millis": 32062,
    "cell_id": "00010-cdb8631c-d681-414d-a0c0-0376cf704e57",
    "execution_start": 1621031670415,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "cols = []\nheight=30\nwidth=(height/2)*len(x_train_arr)\nfig, axes = plt.subplots(4,  max(len(training_files), len(testing_files)), figsize=(width, height))\nfig.suptitle('Data visualisation')\n# Train\nfor i in range(len(x_train_arr)):\n    sns.lineplot(ax=axes[0, i], data=x_train_arr[i].df, x=\" TimeStamp (s)\", y='Pose')\n    sns.histplot(ax=axes[1, i], data=x_train_arr[i].df[\"Pose\"])\n    cols.append(f'Train {key}')    \nfor ax, col in zip(axes[0], cols):\n    ax.set_title(col)\n# Test\ncols=[]\nfor i, key in enumerate(x_test_dict):\n    sns.lineplot(ax=axes[2, i], data=x_test_dict[key], x=\" TimeStamp (s)\", y='Pose')\n    sns.histplot(ax=axes[3, i], data=y_test_dict[key])\n    cols.append(f'Test {key}')\nfor ax, col in zip(axes[2], cols):\n    ax.set_title(col)\nfig.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Adjust dataframes",
   "metadata": {
    "tags": [],
    "cell_id": "00011-19daea0c-8cc0-4af0-8db9-29ed7cca66b6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9fa56c82",
    "execution_millis": 21638652,
    "cell_id": "00012-f9308837-7356-4e2b-87be-d7e05bb6e2a8",
    "execution_start": 1621031702473,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "x_train_numpy = x_train.values\nx_test_numpy = x_test.values\ny_train_numpy = y_train.values\ny_test_numpy = y_test.values",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### GridSearch",
   "metadata": {
    "tags": [],
    "cell_id": "00013-6e77e628-7517-44fc-879b-f3961b3a21f4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## RFC            ",
   "metadata": {
    "tags": [],
    "cell_id": "00015-70d1d196-f232-4f15-859e-79a6d4dee4ee",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Precision",
   "metadata": {
    "tags": [],
    "cell_id": "00017-445395ec-6626-4581-8c19-3d267de067b6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5edfc951",
    "execution_millis": 0,
    "cell_id": "00018-2dddfc4a-88a9-4c83-a191-1a22e92929d4",
    "output_cleared": true,
    "execution_start": 1621031702518,
    "deepnote_cell_type": "code"
   },
   "source": "# Evaluation methods\nimport math\nimport numpy\nfrom sklearn.model_selection import KFold,RepeatedStratifiedKFold, cross_val_score\n\ndef evaluate_model(model, X, y):\n\t# define the evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\t# evaluate the model and collect the results\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\treturn scores\n\n\ndef find_oob_err(max_features=None, max_samples=None, n_estimators=10):\n    rfc = RandomForestClassifier(n_estimators=i, max_features=max_features, oob_score=True, random_state=33, max_samples=max_samples)\n    rfc.fit(x_train, y_train)\n    print(\"rfc oob_score: \", rfc.oob_score_)\n    err = 1-rfc.oob_score_\n    \n    print(\"1-oob_score: \", err)\n    return err\n\n\ndef find_test_accuracy(model, x, y):\n    model.fit(x_train, y_train)\n    print(\"Classification accuracy:\")\n    classification_dict = dict()\n    accuracy_list = list()\n\n    for key in x_test_dict:\n        x_test = x_test_dict[key].drop([' TimeStamp (s)', 'Pose'], axis=1)\n        classifications = model.predict(x_test)\n        annotated_positions = y_test_dict[key].to_numpy()\n        correct_classifications = (classifications == annotated_positions).sum()\n        accuracy_list.append(round(correct_classifications/len(classifications)*100,2))\n        print(f\"{key}: {accuracy_list[-1]}%\")\n        classification_dict[key] = classifications\n\n    avg = round(sum(accuracy_list)/len(accuracy_list),2)\n    print(f\"Average accuracy: {avg}%\\nWith {estimators} estimators and\\nrandom_state={33}\")\n    return avg\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-4383d083-01cd-4dcb-8a59-d136bdc4f88b",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bf301266",
    "execution_start": 1621031702518,
    "execution_millis": 623,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Find best amount of features with cross_validation\ndef get_models_features(n_estimators=10):\n\tmodels = dict()\n\t# explore number of features\n\tfor i in range(1, math.ceil(((AMOUNT_OF_SENSORS*13)+1)/2)):\n\t\tmodels[str(i)] = RandomForestClassifier(max_features=i, n_estimators=n_estimators, random_state=42)\n\treturn models\n\n\nfeature_models = get_models_features()\nfeatures_cross_val_avgs = []\nfor i in range(len(feature_models)):\n    cv_score = evaluate_model(feature_models[str(i+1)], x_train, y_train)\n    features_cross_val_avgs.append(sum(cv_score)/len(cv_score))\nbest_feature_amount = features_cross_val_avgs.index(max(features_cross_val_avgs))+1\nprint(\"best_featue_amount: \", best_feature_amount)\n\n\nplt.plot([i for i in range(1,len(features_cross_val_avgs)+1)], features_cross_val_avgs)\nplt.xlabel(\"Amount of features\")\nplt.ylabel(\"Cross validation average score\")\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00020-7025a21f-b4d7-4cd6-8b54-6db16b957e3f",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "d63b3a2",
    "execution_start": 1620991979756,
    "execution_millis": 1470550,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Find best amount of samples for dataset based on best aount of features\nimport numpy\ndef get_models_samples(n_estimators=10, max_features=None):\n    models = dict()\n    # explore ratios from 10% to 100% in 10% increment\n    for i in numpy.arange(0.1, 1.1, 0.1):\n        key = round(i,1)\n        # set max_samples=None to use 100%\n        if i == 1.0:\n            i = None\n        models[str(i)] = RandomForestClassifier(max_samples=i, random_state=33, n_estimators=n_estimators)\n        print(i)\n    return models\n\nsample_models = get_models_samples()\nsamples_cross_val_avgs = []\nfor samples, model in sample_models.items():\n    cv_score = evaluate_model(model, x_train, y_train)\n    samples_cross_val_avgs.append(sum(cv_score)/len(cv_score))\nbest_sample_amount = list(sample_models.keys())[samples_cross_val_avgs.index(max(samples_cross_val_avgs))]\nprint(\"best_sample_amount: \", best_sample_amount)\n\nplt.plot(list(sample_models.keys()), samples_cross_val_avgs)\nplt.xlabel(\"Percentage of all available samples\")\nplt.ylabel(\"Cross validation average score\")\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00021-937104ec-6985-4fa1-85f7-71c8ce201a76",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "5ff6a6d3",
    "execution_start": 1621002505887,
    "execution_millis": 49,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Find best rfc overall\n\n\"\"\"\ndef get_models_estimators(max_features=None, max_samples=None):\n    models = dict()\n    for i in range(1, 161, 20):\n        models[str(i)] = RandomForestClassifier(max_features=max_features, max_samples=max_samples, n_estimators=i)\n    return models\n\nestimator_models = get_models_estimators(max_features=best_feature_amount, max_samples=best_sample_amount)\nbest_avg = 0\nbest_estimators = -1\nbest_model = None\nfor estimators, model in estimator_models.items():\n    cv_score = evaluate_model(model, x_train, y_train)\n    avg = sum(cv_score)/len(cv_score)\n    if avg > best_avg:\n        best_avg = avg\n        best_estimators = estimators\n        best_model = model\"\"\"\nif best_sample_amount == None or best_sample_amount == \"None\":\n    best_model = RandomForestClassifier(max_features=best_feature_amount, n_estimators=100, random_state=33)\nelse:\n    best_model = RandomForestClassifier(max_features=best_feature_amount, max_samples=best_sample_amount, n_estimators=100, random_state=33)\n\nprint(f\"Best model with {best_feature_amount} features, {best_sample_amount} samplesize and 100 estimators\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00022-4a91d182-07f6-42d5-af4a-2318945e9ca8",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "ff0994f7",
    "execution_start": 1621002507519,
    "execution_millis": 685722,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "# Make final model\nfinal_model = best_model.fit(x_train, y_train)\n\nprint(\"Classification accuracy:\")\nclassification_dict = dict()\naccuracy_list = list()\n\nfor key in x_test_dict:\n    x_test = x_test_dict[key].drop([' TimeStamp (s)', 'Pose'], axis=1)\n    classifications = final_model.predict(x_test)\n    annotated_positions = y_test_dict[key].to_numpy()\n    correct_classifications = (classifications == annotated_positions).sum()\n    accuracy_list.append(round(correct_classifications/len(classifications)*100,2))\n    print(f\"{key}: {accuracy_list[-1]}%\")\n    classification_dict[key] = classifications\n\navg = round(sum(accuracy_list)/len(accuracy_list),2)\nprint(f\"Average accuracy: {avg}%\\nWith 100 estimators and\\nrandom_state={33}\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Visualization\n",
   "metadata": {
    "tags": [],
    "cell_id": "00019-4a4f7b62-06bf-436f-b2c0-9991f5604bb7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "cc265212",
    "execution_millis": 57596,
    "cell_id": "00020-2290f8bb-a07c-46dd-b701-3e87beb42052",
    "execution_start": 1620947739989,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "width=16\nheight=4*len(x_test_dict)\nfig, axes = plt.subplots(len(x_test_dict), 4, figsize=(width, height))\nfor i, key in enumerate(x_test_dict):\n    cols = [f\"{key}: Model classifications\", f\"{key}: Model heatmap\", f\"{key}: Annotated classifications\", f\"{key}: Annotated heatmap\"]\n    df_predict = pd.DataFrame({' Timestamp (s)': x_test_dict[key][' TimeStamp (s)'],'Pose':classification_dict[key]})\n    sns.lineplot(ax=axes[i, 0], data=df_predict,x=' Timestamp (s)',y='Pose')\n    sns.heatmap(ax=axes[i, 1], data=confusion_matrix(y_test_dict[key], classification_dict[key]), cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n    sns.lineplot(ax=axes[i, 2], data=x_test_dict[key], x=\" TimeStamp (s)\", y='Pose')\n    sns.heatmap(ax=axes[i, 3], data=confusion_matrix(y_test_dict[key], y_test_dict[key].to_numpy()), cmap=\"YlGnBu\", annot=True, fmt=\"d\")\n    for ax, col in zip(axes[i], cols): ax.set_title(col)\nfig.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Classification matrix and report",
   "metadata": {
    "tags": [],
    "cell_id": "00021-80275097-f0d2-461f-a5b1-849df0309c1d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "fae3ec62",
    "execution_millis": 119,
    "cell_id": "00023-382afae5-a408-4dac-baa9-27450d32e45a",
    "execution_start": 1620947797576,
    "output_cleared": true,
    "deepnote_cell_type": "code"
   },
   "source": "from joblib import dump, load\ndump(final_model, f'../models/RFC_model_{AMOUNT_OF_SENSORS}.joblib')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=59d486bc-e14d-4632-9064-12272fc72d11' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f4fde45515710cbe4f4cf44a8ddef1b298277709bd6c5462499553af68a98f2e"
    }
   }
  },
  "deepnote_notebook_id": "5b420c0c-e067-45cb-85b6-bd9590caa837",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}